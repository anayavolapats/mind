<html>
<head>
<title>m11.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #0033b3;}
.s1 { color: #080808;}
.s2 { color: #067d17;}
.s3 { color: #8c8c8c; font-style: italic;}
.s4 { color: #1750eb;}
</style>
</head>
<body bgcolor="#ffffff">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#c0c0c0" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
m11.py</font>
</center></td></tr></table>
<pre><span class="s0">import </span><span class="s1">cv2</span>
<span class="s0">from </span><span class="s1">deepface </span><span class="s0">import </span><span class="s1">DeepFace</span>
<span class="s0">from </span><span class="s1">gtts </span><span class="s0">import </span><span class="s1">gTTS</span>
<span class="s0">import </span><span class="s1">os</span>
<span class="s0">import </span><span class="s1">pygame</span>
<span class="s0">import </span><span class="s1">time</span>

<span class="s0">def </span><span class="s1">speak(text):</span>
    <span class="s1">tts = gTTS(text=text, lang=</span><span class="s2">'en'</span><span class="s1">)</span>
    <span class="s1">tts.save(</span><span class="s2">&quot;emotion.mp3&quot;</span><span class="s1">)</span>
    <span class="s1">pygame.mixer.music.load(</span><span class="s2">&quot;emotion.mp3&quot;</span><span class="s1">)</span>
    <span class="s1">pygame.mixer.music.play()</span>

<span class="s0">def </span><span class="s1">main():</span>
    <span class="s3"># Initialize Pygame for audio playback</span>
    <span class="s1">pygame.init()</span>
    <span class="s1">pygame.mixer.init()</span>

    <span class="s3"># Print the path to cascade XML file for debugging</span>
    <span class="s1">print(cv2.data.haarcascades + </span><span class="s2">'haarcascade_frontalface_default.xml'</span><span class="s1">)</span>

    <span class="s3"># Load face cascade classifier</span>
    <span class="s1">face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + </span><span class="s2">'haarcascade_frontalface_default.xml'</span><span class="s1">)</span>

    <span class="s3"># Start capturing video from webcam (change the index as needed)</span>
    <span class="s1">cap_webcam = cv2.VideoCapture(</span><span class="s4">0</span><span class="s1">)</span>

    <span class="s3"># Open a pre-saved video file (replace with your video file path)</span>
    <span class="s1">video_file_path = </span><span class="s2">&quot;/Users/yanatsapalova/hci_lab/restarted/media/emotions.mp4&quot;</span>
    <span class="s1">cap_video = cv2.VideoCapture(video_file_path)</span>

    <span class="s3"># Check if both captures are opened successfully</span>
    <span class="s0">if not </span><span class="s1">cap_webcam.isOpened() </span><span class="s0">or not </span><span class="s1">cap_video.isOpened():</span>
        <span class="s1">print(</span><span class="s2">&quot;Error: One or more video streams could not be opened.&quot;</span><span class="s1">)</span>
        <span class="s0">return</span>

    <span class="s1">speak_counter = </span><span class="s4">0  </span><span class="s3"># Counter to limit TTS frequency</span>
    <span class="s1">swap_counter = </span><span class="s4">0  </span><span class="s3"># Counter to control screen swapping</span>

    <span class="s0">while True</span><span class="s1">:</span>
        <span class="s3"># Capture frame from webcam</span>
        <span class="s1">ret_webcam, frame_webcam = cap_webcam.read()</span>

        <span class="s3"># Capture frame from pre-saved video</span>
        <span class="s1">ret_video, frame_video = cap_video.read()</span>

        <span class="s0">if not </span><span class="s1">ret_webcam </span><span class="s0">or not </span><span class="s1">ret_video:</span>
            <span class="s0">break</span>

        <span class="s3"># Convert frames to grayscale for emotion detection</span>
        <span class="s1">gray_frame_webcam = cv2.cvtColor(frame_webcam, cv2.COLOR_BGR2GRAY)</span>
        <span class="s1">rgb_frame_webcam = cv2.cvtColor(gray_frame_webcam, cv2.COLOR_GRAY2RGB)</span>

        <span class="s3"># Detect faces in the webcam frame</span>
        <span class="s1">faces_webcam = face_cascade.detectMultiScale(gray_frame_webcam, scaleFactor=</span><span class="s4">1.1</span><span class="s1">, minNeighbors=</span><span class="s4">5</span><span class="s1">, minSize=(</span><span class="s4">30</span><span class="s1">, </span><span class="s4">30</span><span class="s1">))</span>

        <span class="s0">for </span><span class="s1">(x, y, w, h) </span><span class="s0">in </span><span class="s1">faces_webcam:</span>
            <span class="s1">face_roi_webcam = rgb_frame_webcam[y:y + h, x:x + w]</span>
            <span class="s1">result_webcam = DeepFace.analyze(face_roi_webcam, actions=[</span><span class="s2">'emotion'</span><span class="s1">], enforce_detection=</span><span class="s0">False</span><span class="s1">)</span>
            <span class="s1">emotion_webcam = result_webcam[</span><span class="s4">0</span><span class="s1">][</span><span class="s2">'dominant_emotion'</span><span class="s1">]</span>

            <span class="s3"># Draw rectangle around face and label with predicted emotion</span>
            <span class="s1">cv2.rectangle(frame_webcam, (x, y), (x + w, y + h), (</span><span class="s4">0</span><span class="s1">, </span><span class="s4">0</span><span class="s1">, </span><span class="s4">255</span><span class="s1">), </span><span class="s4">2</span><span class="s1">)</span>
            <span class="s1">cv2.putText(frame_webcam, emotion_webcam, (x, y - </span><span class="s4">10</span><span class="s1">), cv2.FONT_HERSHEY_SIMPLEX, </span><span class="s4">0.9</span><span class="s1">, (</span><span class="s4">0</span><span class="s1">, </span><span class="s4">0</span><span class="s1">, </span><span class="s4">255</span><span class="s1">), </span><span class="s4">2</span><span class="s1">)</span>

            <span class="s3"># Speak the detected emotion less frequently</span>
            <span class="s0">if </span><span class="s1">speak_counter % </span><span class="s4">50 </span><span class="s1">== </span><span class="s4">0</span><span class="s1">:  </span><span class="s3"># Adjust this value to control frequency</span>
                <span class="s1">speak(emotion_webcam)</span>

        <span class="s1">speak_counter += </span><span class="s4">1</span>

        <span class="s3"># Resize the pre-saved video frame to be smaller</span>
        <span class="s1">height, width, _ = frame_webcam.shape</span>
        <span class="s1">small_frame_video = cv2.resize(frame_video, (width // </span><span class="s4">3</span><span class="s1">, height // </span><span class="s4">3</span><span class="s1">))</span>

        <span class="s3"># Swap the screens every few frames</span>
        <span class="s0">if </span><span class="s1">swap_counter % </span><span class="s4">100 </span><span class="s1">== </span><span class="s4">0</span><span class="s1">:  </span><span class="s3"># Adjust this value to control swap frequency</span>
            <span class="s1">main_frame, overlay_frame = frame_webcam, small_frame_video</span>
        <span class="s0">else</span><span class="s1">:</span>
            <span class="s1">main_frame, overlay_frame = frame_video, cv2.resize(frame_webcam, (width // </span><span class="s4">3</span><span class="s1">, height // </span><span class="s4">3</span><span class="s1">))</span>

        <span class="s1">swap_counter += </span><span class="s4">1</span>

        <span class="s3"># Overlay the small video frame on the bottom-right corner of the main frame</span>
        <span class="s1">main_frame[-overlay_frame.shape[</span><span class="s4">0</span><span class="s1">]:, -overlay_frame.shape[</span><span class="s4">1</span><span class="s1">]:] = overlay_frame</span>

        <span class="s3"># Display the combined frame in fullscreen</span>
        <span class="s1">cv2.namedWindow(</span><span class="s2">'Combined'</span><span class="s1">, cv2.WND_PROP_FULLSCREEN)</span>
        <span class="s1">cv2.setWindowProperty(</span><span class="s2">'Combined'</span><span class="s1">, cv2.WND_PROP_FULLSCREEN, cv2.WINDOW_FULLSCREEN)</span>
        <span class="s1">cv2.imshow(</span><span class="s2">'Combined'</span><span class="s1">, main_frame)</span>

        <span class="s3"># Check for exit key press</span>
        <span class="s0">if </span><span class="s1">cv2.waitKey(</span><span class="s4">1</span><span class="s1">) &amp; </span><span class="s4">0xFF </span><span class="s1">== ord(</span><span class="s2">'q'</span><span class="s1">):</span>
            <span class="s0">break</span>

    <span class="s3"># Release video capture objects and close all windows</span>
    <span class="s1">cap_webcam.release()</span>
    <span class="s1">cap_video.release()</span>
    <span class="s1">cv2.destroyAllWindows()</span>
    <span class="s1">pygame.mixer.quit()</span>

<span class="s0">if </span><span class="s1">__name__ == </span><span class="s2">&quot;__main__&quot;</span><span class="s1">:</span>
    <span class="s1">main()</span>
</pre>
</body>
</html>